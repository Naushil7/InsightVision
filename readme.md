# InsightVision

![Status](https://img.shields.io/badge/status-deployed-brightgreen) ![License](https://img.shields.io/badge/license-MIT-blue) ![GCP](https://img.shields.io/badge/deployed%20on-Google%20Cloud%20Run-orange)

ğŸ”— **Live App**: [https://insightvision-18564642412.us-central1.run.app](https://insightvision-18564642412.us-central1.run.app)

> InsightVision is a lightweight, cloud-deployable document understanding tool that extracts content from PDFs or images, semantically indexes it using embeddings, and allows LLM-powered question answering using Mistral.

---

## ğŸ§  Project Explanation

### ğŸ¤” Why Not Just Use an LLM Directly on PDFs?

While LLMs like OpenAIâ€™s GPT-4 can ingest PDFs directly, InsightVision is built with a more **structured and scalable pipeline** where the LLM plays a focused role:

- âœ… The **LLM is not responsible for everything** â€” itâ€™s only used at the end for answering questions.
- ğŸ” All the **heavy lifting (text extraction, cleaning, chunking, semantic search)** is done **before** the LLM is called.
- ğŸ§  This means:
  - Lower token usage (cheaper, faster)
  - More contextually relevant answers
  - Better explainability and modularity

In contrast, uploading a PDF to an LLM treats the model as a black box â€” which might work for summaries, but falls short for precise querying, scalable architecture, and image-based documents.

While it's tempting to directly send an entire PDF to an LLM, this approach has major limitations:

- **Token Limits**: Most LLM APIs have strict context length restrictions (e.g., 4K to 32K tokens). Long PDFs easily exceed this.
- **No Indexing**: LLMs don't inherently "understand" document structure or let you query specific parts semantically.
- **OCR Blindness**: LLMs cannot process raw image scans or scanned text in PDFs.
- **No Reuse**: Each query is a cold start â€” there's no persistent understanding or index.

**InsightVision solves all of these** by:
- Extracting and chunking text intelligently
- Using **FAISS** to find only the most relevant content
- Feeding only top-matching chunks into the LLM
- Supporting **images and scanned PDFs** via OCR
- Running offline-ready with local embeddings

This pipeline is faster, smarter, more scalable, and cost-efficient than raw LLM usage.


InsightVision was created to combine robust document understanding with the power of large language models â€” especially for analyzing resumes, scanned documents, academic PDFs, and images.

### What it does:
- Allows users to upload PDFs and images
- Extracts high-quality text (with OCR fallback for images)
- Embeds the extracted content using `SentenceTransformer`
- Stores and searches via FAISS for semantic similarity
- Passes top-matched context to the **Mistral API** to answer questions
- Deployed as a **serverless Streamlit app on Google Cloud Run**

This project emphasizes modularity, offline resilience (local model), and clean UI/UX via Streamlit. Itâ€™s designed to be both a robust ML pipeline and a showcase-ready portfolio project.

---

## ğŸ–¼ï¸ Dashboard Preview

![InsightVision UI](data/temp/Dashborad_Image.png)

---

## ğŸš€ Features

- ğŸ“„ PDF and ğŸ–¼ï¸ image (OCR) support
- ğŸ” FAISS-based vector search for semantic matching
- ğŸ¤– Mistral LLM integration for intelligent answers
- ğŸ§  Smart OCR fallback and captioning-ready design
- â˜ï¸ Dockerized & deployable on GCP (Cloud Run)
- ğŸ–¥ï¸ Streamlit frontend

---

## ğŸ“¦ Folder Structure

```bash
insightvision/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # Streamlit frontend
â”‚   â”œâ”€â”€ config.py                # Environment setup
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ extract/             # PDF and image handlers
â”‚   â”‚   â”œâ”€â”€ vectorstore/         # Chunking, embedding, FAISS
â”‚   â”‚   â””â”€â”€ llm/                 # Mistral API integration
â”‚   â”œâ”€â”€ routes/                  # (Optional FastAPI backend)
â”‚   â”œâ”€â”€ utils/                   # file_ops, helpers
â”œâ”€â”€ models/                     # Pre-downloaded embedding model
â”œâ”€â”€ data/                       # Uploaded files + FAISS index
â”œâ”€â”€ .env                        # API keys and configs
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ run_local.sh
```

---

## ğŸ§  Project Workflow (Mermaid)

```mermaid
flowchart TD
  A[User Uploads File] --> B{File Type?}
  B -->|PDF| C[Extract PDF Text]
  B -->|Image| D[OCR Extraction]
  C & D --> E[Chunk & Embed]
  E --> F[FAISS Index/Search]
  G[User Asks Question] --> F
  F --> H[Retrieve Top Matches]
  H --> I[Mistral API]
  I --> J[Answer Shown on UI]
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repo
```bash
git clone https://github.com/yourusername/InsightVision.git
cd InsightVision
```

### 2. Setup Environment
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 3. Add `.env`
```env
MISTRAL_API_KEY=your-api-key
UPLOAD_DIR=data/uploads
INDEX_PATH=data/vectorstore/index.faiss
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

### 4. Run Locally
```bash
streamlit run app/main.py
```

---

## ğŸ³ Docker Instructions

### Build Locally
```bash
docker build -t insightvision .
```

### Run Locally
```bash
docker run -p 8080:8080 --env-file .env insightvision
```

### Deploy to GCP
```bash
gcloud run deploy insightvision \
  --image us-central1-docker.pkg.dev/YOUR_PROJECT/insightvision-repo/insightvision \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --port 8080 \
  --memory 1Gi \
  --set-env-vars MISTRAL_API_KEY=your-key
```

---

## ğŸ¤ Future Enhancements
- ğŸ”„ Add support for BLIP-based image captioning
- ğŸ§© Plug-in integration for LangChain or ChromaDB
- ğŸ§  Citation generation using RAG
- ğŸ—‚ï¸ Multi-file ingestion and Q&A

---

## ğŸ“„ License

MIT License. Use freely and contribute back!

---